{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cybersecurity-excellence-awards.com/wp-content/uploads/2017/06/366812.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center>Darwin Forecasting Model Building </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior to getting started:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darwin notebook will no longer support 'Register User' starting from 2.0. As a user, you must have credentials ready before using this notebook. \n",
    "\n",
    "In order to proceed, in the Environment Variables cell: \n",
    "1. Set your username and password to ensure that you're able to log in successfully\n",
    "2. Set the path to the location of your datasets if you are using your own data.  The path is set for examples.\n",
    "  <br><b>NOTE:</b> We provide two ways to analyze feature importance. One is to use the entire dataset; the other one is to analyze a few samples to understand individual samples. In the latter case, we advise users to use a small dataset (<=500) because it takes long time to process individual samples. \n",
    "\n",
    "Here are a few things to be mindful of:\n",
    "1. For every run, check the job status (i.e. requested, failed, running, completed) and wait for job to complete before proceeding. \n",
    "2. If you're not satisfied with your model and think that Darwin can benefit from extra training, use the resume function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "from amb_sdk.sdk import DarwinSdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Darwin SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DarwinSdk()\n",
    "ds.set_url('https://darwin-api.sparkcognition.com/v1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set your user id and password accordingly\n",
    "USER=\"[your Darwin user id]\"\n",
    "PW=\"[your Darwin password]\"\n",
    "\n",
    "# Set path to datasets - The default below assumes Jupyter was started from amb-sdk/examples/Enterprise/\n",
    "# Modify accordingly if you wish to use your own data\n",
    "PATH_TO_DATASET = '../../sets/'\n",
    "TRAIN_DATASET = 'sine_train.csv'\n",
    "TEST_DATASET = 'sine_test.csv'\n",
    "\n",
    "# A timestamp is used to create a unique name in the event you execute the workflow multiple times or with \n",
    "# different datasets.  File names must be unique in Darwin.\n",
    "ts = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, msg = ds.auth_login_user(USER, PW)\n",
    "if not status:\n",
    "    print(msg)\n",
    "else:\n",
    "    print('You are logged in.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read dataset and view a file snippet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(PATH_TO_DATASET, TRAIN_DATASET))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload training dataset to Darwin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status, dataset = ds.upload_dataset(os.path.join(PATH_TO_DATASET, TRAIN_DATASET))\n",
    "if not status:\n",
    "    print(dataset)\n",
    "else:\n",
    "    print('Data is successfully uploaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Upload testing dataset to Darwin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, dataset = ds.upload_dataset(os.path.join(PATH_TO_DATASET, TEST_DATASET))\n",
    "if not status:\n",
    "    print(dataset)\n",
    "else:\n",
    "    print('Data is successfully uploaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Data\n",
    "Analyze data is a necessary step before cleaning data and creating model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, analyze_id = ds.analyze_data(TRAIN_DATASET, \n",
    "                                     job_name = 'Darwin_analyze_data_job' + \"-\" + ts, \n",
    "                                     artifact_name = 'Darwin_analyze_data_artifact' + \"-\" + ts)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job('Darwin_analyze_data_job' + \"-\" + ts)\n",
    "else:\n",
    "    print(analyze_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.lookup_job_status_name(analyze_id['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "\n",
    "Starting in Version 1.6, Darwin SDK offers a way to clean your data outside of model training. Every dataset needs to be cleaned before creating a model. There is no need to save the cleaned data and upload it, but users need to specify the target name before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'y'\n",
    "status, job_id = ds.clean_data(dataset_name=TRAIN_DATASET, target=target)\n",
    "if not status:\n",
    "    print(job_id)\n",
    "else:\n",
    "    print('Data has been successfully cleaned!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, specify the parameters used to create the forecasting model:\n",
    "- `model`: the name of your model\n",
    "- `forecast_horizon`: the forecast length, which is how far you need to predict in the future, int type\n",
    "- `max_train_time`: the amount of time used for training (possibly shorter with early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())\n",
    "model = \"model\" + \"-\" + ts\n",
    "forecast_horizon = 2\n",
    "max_train_time = '00:02'\n",
    "status, job_id = ds.create_model(dataset_names=TRAIN_DATASET,\n",
    "                                 model_name=model,\n",
    "                                 forecast_horizon=forecast_horizon,\n",
    "                                 fit_profile_name=job_id['profile_name'],\n",
    "                                 max_train_time=max_train_time)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up job status\n",
    "ds.lookup_job_status_name(job_id['job_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up the model details\n",
    "model_info = ds.lookup_model_name(job_id['model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Training (Optional)\n",
    "Run the following cell for extra training, specify the amount of time for extra training using `max_train_time` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_time = '00:01'\n",
    "status, job_id = ds.resume_training_model(dataset_names=TRAIN_DATASET,\n",
    "                                          model_name=model,\n",
    "                                          max_train_time=max_train_time)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Run the following cell for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataset\n",
    "status, job_id = ds.clean_data(TEST_DATASET, model_name=model)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "status, artifact = ds.run_model(dataset_name=TEST_DATASET, \n",
    "                                model_name=model, \n",
    "                                forecast_horizon=forecast_horizon)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View prediction\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in test dataset\n",
    "df = pd.read_csv(os.path.join(PATH_TO_DATASET, TEST_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot against predictions\n",
    "seq_len = model_info[1]['description']['best_genome'][0]['layer 1']['parameters']['seqlength']\n",
    "preds = prediction.iloc[::forecast_horizon, :].values.reshape(-1)\n",
    "plt.rcParams['figure.figsize']= 20,20\n",
    "plt.plot(df[target][seq_len:].reset_index(drop=True), color='b')\n",
    "plt.plot(preds, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
